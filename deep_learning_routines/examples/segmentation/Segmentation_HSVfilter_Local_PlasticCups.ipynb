{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance segmentation of plastic cups in a video\n",
    "\n",
    "This notebook demonstrates the task of instance segmentation in one video (on all frames), in this case segmentation of a plastic cup that is transported through the water column. The segmentation is based on creating a mask with a blue-color filter. In this project there is no .. for distortion related to the camera lens. The arucos are not used with their accompanying x,y,z coordinates, but just as markers in a local coordinate system in a x,y space (2D). \n",
    "\n",
    "General information: \n",
    "\n",
    "<br>\n",
    "\n",
    "| Model | Storage (datasets + model) | Environment |\n",
    "| ----- | ---- | ---- |\n",
    "| HSV filter in OpenCV | P-drive | Locally on your own computer\n",
    "\n",
    "<br>\n",
    "\n",
    "The notebook follows these steps:\n",
    "\n",
    "1. General setup\n",
    "2. Settings\n",
    "3. Pre-processing \n",
    "4. Creating and tailoring the segmentation routine\n",
    "5. Using the tailored segmentation routine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import cv2.aruco as aruco\n",
    "import imutils\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import time\n",
    "import collections\n",
    "\n",
    "# Enabling interactive Matplotlib widgets to be used to tailor the segmentation routine\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full path to the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathVideo = r\"P:\\11209190-023-plasticsinwatercolumn\\02_measurements\\02_PTV\\01_flow_measurements\\B3L0_CAM01_20231211_13-21-52.avi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings to tailor the segmentation routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Area of Interest (AOI) and the HSV range for the segmentation\n",
    "starting_frame_tailor = 1490  # Frame at which the segmentation is tailored\n",
    "AOI_1 = [120, 2600, 100, 2130]  # [y1,y2,x1,x2]\n",
    "HSV_range = [[100, 100, 100], [120, 255, 255]]  # [lower, upper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings used in the segmentation routine, can be left as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the frame at which the processing (segmentation routine) should start\n",
    "starting_frame_use = 1000\n",
    "\n",
    "# Define frame synchronization ID and detection parameters\n",
    "FrameIDSync = starting_frame_use  # Set the initial frame ID for synchronization\n",
    "DTskip = 5  # Number of frames to skip if the particle is not detected\n",
    "MemoryDetection = collections.deque(maxlen=30)  # Memory for particle detection in the last 30 frames\n",
    "\n",
    "# Output directory for the processed frames\n",
    "dir_output = r\"P:\\11209190-023-plasticsinwatercolumn\\03_processing\\2DPTV_detectionresults\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video file\n",
    "cap = cv2.VideoCapture(pathVideo)\n",
    "\n",
    "# Get and print the total frame count and FPS\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "print(\"FrameCount:\", int(frame_count), \"\\nFrames per second:\", int(fps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Create an Area of Interest (AOI) mask\n",
    "\n",
    "To omit certain parts of the frame (in this case the blue bottom of the frame). This AOI can be different for each video, so it was defined manually in Settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first frame from the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Get the height and width of the frame\n",
    "h, w = frame.shape[:2]\n",
    "\n",
    "# Check if the video is recorded in portrait mode\n",
    "rotate = False\n",
    "if h < w:\n",
    "    rotate = True\n",
    "    # Rotate the frame 90 degrees clockwise\n",
    "    frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "    # Update the height and width after rotation\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "# Create an AOI mask\n",
    "AOI = np.zeros(frame.shape[:2])\n",
    "AOI[AOI_1[0] : AOI_1[1], AOI_1[2] : AOI_1[3]] = 1\n",
    "AOI = AOI.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Calibration of the camera "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to detect and annotate ARUCO markers in a given frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotCalibration(frame_gray, frame_display):\n",
    "    global aruco_dic, parameters_aruco\n",
    "    # Detect ARUCO markers in the grayscale frame\n",
    "    corners, ids, rejectedImgPoints = aruco.detectMarkers(\n",
    "        frame_gray, aruco_dict, parameters=parameters_aruco\n",
    "    )\n",
    "    ids_f = []\n",
    "    corners_f = []\n",
    "    # Iterate through detected marker IDs\n",
    "    for i, id_i in enumerate(ids):\n",
    "        if id_i in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]:  # Center of the ARUCO markers\n",
    "            pts = corners[i].astype(int).reshape((-1, 1, 2))\n",
    "            # Draw a circle at the center of the marker\n",
    "            cv2.circle(\n",
    "                frame_display, tuple(pts.mean(axis=0)[0].astype(int)), 1, (0, 255, 0), 2\n",
    "            )\n",
    "            # Annotate the marker ID\n",
    "            cv2.putText(\n",
    "                frame_display,\n",
    "                f\"Id_{id_i}\",\n",
    "                (pts[0, 0][0], pts[0, 0][1]),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA,\n",
    "            )\n",
    "            ids_f.append(id_i)\n",
    "            corners_f.append(corners[i])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function uses the ARUCO dictionary and parameters defined to detect markers and draw circles and text annotations on the detected markers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARUCO parameterization (define ARUCO dictionary and parameters)\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_4X4_1000)  # Loading ARUCO fiduciary marker dictionary (4x4 code)\n",
    "\n",
    "# Initialize ARUCO detector parameters (equal for each video)\n",
    "parameters_aruco = aruco.DetectorParameters()\n",
    "parameters_aruco.adaptiveThreshWinSizeMin = 100  # Minimum window size for adaptive thresholding\n",
    "parameters_aruco.adaptiveThreshWinSizeMax = 300  # Maximum window size for adaptive thresholding\n",
    "parameters_aruco.adaptiveThreshConstant = 5  # Constant subtracted from mean or weighted mean\n",
    "parameters_aruco.minMarkerPerimeterRate = 0.03  # Minimum perimeter rate of marker\n",
    "parameters_aruco.maxMarkerPerimeterRate = 4.0  # Maximum perimeter rate of marker\n",
    "parameters_aruco.polygonalApproxAccuracyRate = 0.03  # Polygonal approximation accuracy rate\n",
    "parameters_aruco.minCornerDistanceRate = 0.1  # Minimum distance between corners\n",
    "parameters_aruco.minDistanceToBorder = 3  # Minimum distance to image border\n",
    "parameters_aruco.minMarkerDistanceRate = 0.1  # Minimum distance between markers\n",
    "parameters_aruco.cornerRefinementMethod = aruco.CORNER_REFINE_SUBPIX  # Corner refinement method\n",
    "parameters_aruco.cornerRefinementWinSize = 5  # Window size for corner refinement\n",
    "parameters_aruco.cornerRefinementMaxIterations = 30  # Maximum iterations for corner refinement\n",
    "parameters_aruco.cornerRefinementMinAccuracy = 0.1  # Minimum accuracy for corner refinement\n",
    "parameters_aruco.markerBorderBits = 1  # Number of bits in marker borders\n",
    "parameters_aruco.perspectiveRemovePixelPerCell = 8  # Perspective removal pixel per cell\n",
    "parameters_aruco.perspectiveRemoveIgnoredMarginPerCell = 0.13  # Perspective removal ignored margin per cell\n",
    "parameters_aruco.maxErroneousBitsInBorderRate = 0.35  # Maximum erroneous bits in border rate\n",
    "parameters_aruco.errorCorrectionRate = 0.6  # Error correction rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tailor segmentation routine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process the video and detect the ARUCO markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set video frame position\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, starting_frame_tailor)  # Set the video position to the 1490th frame (index base 0)\n",
    "ret, frame = cap.read()  # Read the frame at the specified position\n",
    "\n",
    "# Rotate the frame 90 degrees clockwise if needed\n",
    "if rotate:\n",
    "    frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)  # Rotate the frame 90 degrees clockwise if needed\n",
    "\n",
    "# Prepare the frame for processing\n",
    "frame_display = frame\n",
    "frame_HSV = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)  # Convert the frame to HSV color space\n",
    "frame_GRAY = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert the frame to grayscale\n",
    "\n",
    "# Detect and annotate ARUCO markers\n",
    "PlotCalibration(frame_GRAY, frame_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect blue objects in the frames and draw its contours and area "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color substraction to detect blue objects\n",
    "fgMask = cv2.inRange(frame_HSV, np.array(HSV_range[0]), np.array(HSV_range[1]))\n",
    "fgMask = cv2.blur(fgMask, (5, 5), 0)\n",
    "\n",
    "# Apply AOI mask\n",
    "fgMask = cv2.bitwise_and(fgMask, fgMask, mask=AOI)\n",
    "\n",
    "# Detect Main Blob that is filtered with the color substraction\n",
    "contours_Background, _ = cv2.findContours(fgMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "if len(contours_Background) > 0:\n",
    "    for i, contour in enumerate(contours_Background):\n",
    "        ((x, y), radius) = cv2.minEnclosingCircle(contour)\n",
    "        area = cv2.contourArea(contour)\n",
    "        try:\n",
    "            M = cv2.moments(contour)\n",
    "            center = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
    "            # Print the area and the radius of the blob\n",
    "            cv2.putText(\n",
    "                frame_display,\n",
    "                f\"{area}_{radius}\",\n",
    "                (center[0] + 5, center[1] + 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA,\n",
    "            )\n",
    "            ObjectDetected = True\n",
    "            cv2.drawContours(frame_display, contour, -1, (0, 255, 0), 3)\n",
    "            cv2.circle(frame_display, center, 5, (0, 255, 0), 2)\n",
    "        except:\n",
    "            None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results in an interactive Matplotlib window to tailor the segmentation routine (AOI, color range, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(frame_display[:,:,::-1]) # Display the annotated RGB frame\n",
    "plt.figure()\n",
    "plt.imshow(frame_HSV) # Display the HSV frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run segmentation routine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process the video by setting the starting frame, rotating if necessary and copying it for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the process time\n",
    "startP = time.process_time()\n",
    "\n",
    "# Initialize results list\n",
    "Results = []\n",
    "\n",
    "# Set video frame position\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, starting_frame_use)  # Set the video position to the starting frame defined in Settings (index base 0)\n",
    "ret, frame = cap.read()  # Read the frame at the specified position\n",
    "\n",
    "# Rotate the frame if necessary\n",
    "if rotate:\n",
    "    frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "# Prepare frame for display and processing\n",
    "frame_display = frame.copy()  # Copy the frame for display\n",
    "frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert the frame to grayscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks if a frame includes a blue object, if this is not the case for 30 consequetive frames, it skips 5 frames and re-analyzes. If a blue object is detected, it's contour and area is drawn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize object detection flag\n",
    "ObjectDetected = False\n",
    "\n",
    "# Start the main loop\n",
    "while True:\n",
    "    ObjectDetected = False  # Reset object detection flag\n",
    "    start = time.process_time()  # Start the process time for the current frame\n",
    "    surfaced = 0  # Initialize surfaced variable\n",
    "    ret, frame = cap.read()  # Read the next frame from the video\n",
    "\n",
    "    if ret:\n",
    "        # Rotate the frame if necessary\n",
    "        if rotate:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "        # Prepare frame for display and processing\n",
    "        frame_display = frame.copy()  # Copy the frame for display\n",
    "        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert the frame to grayscale\n",
    "        frame_HSV = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)  # Convert the frame to HSV color space\n",
    "\n",
    "        # Detect and annotate ARUCO markers\n",
    "        PlotCalibration(frame_GRAY, frame_display)\n",
    "\n",
    "        # Color subtraction: blue\n",
    "        fgMask = cv2.inRange(frame_HSV, np.array(HSV_range[0]), np.array(HSV_range[1]))  # Create a mask for blue color\n",
    "        fgMask = cv2.blur(fgMask, (5, 5), 0)  # Blur the mask to reduce noise\n",
    "        fgMask = cv2.bitwise_and(fgMask, fgMask, mask=AOI)  # Apply the AOI mask to the color mask\n",
    "\n",
    "        # Detect main blob\n",
    "        contours_Background, _ = cv2.findContours(fgMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)  # Find contours in the mask\n",
    "\n",
    "        if len(contours_Background) > 0:\n",
    "            for i, contour in enumerate(contours_Background):\n",
    "                ((x, y), radius) = cv2.minEnclosingCircle(contour)  # Get the minimum enclosing circle for the contour\n",
    "\n",
    "                # Check if the contour is within the AOI and has a valid radius\n",
    "                if (\n",
    "                    (AOI_1[0] < y)\n",
    "                    and (y < AOI_1[1])\n",
    "                    and (AOI_1[2] < x)\n",
    "                    and (x < AOI_1[3])\n",
    "                    and (radius > 50)\n",
    "                ):\n",
    "                    area = cv2.contourArea(contour)  # Calculate the area of the contour\n",
    "                    if (area > 10000) & (area < 400000):\n",
    "                        M = cv2.moments(contour)  # Calculate moments of the contour\n",
    "                        center = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))  # Calculate the center of the contour\n",
    "                        ObjectDetected = True  # Set object detection flag to True\n",
    "                        cv2.drawContours(frame_display, contour, -1, (0, 255, 0), 6)  # Draw the contour on the display frame\n",
    "                        cv2.circle(frame_display, center, 5, (0, 255, 0), 2)  # Draw the center of the contour\n",
    "\n",
    "        if not ObjectDetected:\n",
    "            center = [np.nan, np.nan]  # Set center to NaN if no object is detected\n",
    "\n",
    "        # Display frame information\n",
    "        cv2.putText(\n",
    "            frame_display,\n",
    "            r\"Frame {:0}\".format(cap.get(cv2.CAP_PROP_POS_FRAMES)),\n",
    "            (30, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2, cv2.LINE_AA,\n",
    "        )\n",
    "        time_diff = time.process_time() - start\n",
    "        fps_actual = 1 / time_diff if time_diff > 0 else float('inf')\n",
    "        cv2.putText(\n",
    "            frame_display,\n",
    "            r\"fps {:0.2f}\".format(fps_actual),\n",
    "            (30, 70),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2, cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "        # Resize and display the frame\n",
    "        frame_display = imutils.resize(frame_display, width=500)\n",
    "        cv2.imshow(\"Frame\", frame_display)\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        # Append results\n",
    "        Results.append([FrameIDSync, center[0], center[1]])\n",
    "\n",
    "        # Skip frames if no particle is detected\n",
    "        MemoryDetection.append(ObjectDetected)\n",
    "        if (not any(MemoryDetection)) & (len(MemoryDetection) == 30):\n",
    "            print(f\"skipping {DTskip} frames\")\n",
    "            FrameIDSync += DTskip\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, FrameIDSync)\n",
    "        else:\n",
    "            FrameIDSync = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "\n",
    "        # Break the loop if the end of the video is reached\n",
    "        if FrameIDSync >= frame_count:\n",
    "            break\n",
    "\n",
    "# Clean up and print results\n",
    "cv2.destroyAllWindows()\n",
    "Results = pd.DataFrame(Results, columns=[\"FrameIDSync\", \"centre_x\", \"centre_y\"])\n",
    "print(\"Processing time: \", time.process_time() - startP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are saved as a csv file to a defined directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the name of the video file without extension\n",
    "nameexp = pathVideo.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "outputpath = os.path.join(dir_output, nameexp)\n",
    "if not os.path.exists(outputpath):\n",
    "    os.makedirs(outputpath)\n",
    "\n",
    "# Save the results to a CSV file in the output directory\n",
    "Results.to_csv(os.path.join(outputpath, nameexp + \"_results.txt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scikit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
