{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"C:\\\\Users\\\\gerritsm\\\\Github\\\\sam2\\\\checkpoints\\\\sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"C:\\\\Users\\\\gerritsm\\\\Github\\\\sam2\\\\sam2\\\\configs\\\\sam2.1\\\\sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `frames_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
    "frames_dir = \".\\\\atlanticFlumeTests\\\\video_20241212_144325_jpg_test\"\n",
    "\n",
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(frames_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "inference_state = predictor.init_state(video_path=frames_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEGMENT AND TRACK OBJECT\n",
    "#reset predictor\n",
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Define point(s) as a prompt\n",
    "points = np.array([[250, 980], [950, 850], [1750, 1000], [2000, 780]], dtype=np.float32)\n",
    "labels = np.array([1, 1, 1, 1], np.int32) # 0: negative 1: positive labels\n",
    "\n",
    "# Conditioning\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(frames_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Assuming video_segments, frame_names, and frames_dir are already defined\n",
    "vis_frame_stride = 1\n",
    "output_video_path = \"p:\\\\computervisiondeltares\\\\testing_CNNprotocol\\\\atlanticFlumeTests\\\\output_{}.mp4\".format(os.path.basename(frames_dir))\n",
    "frame_rate = 10  # Define the frame rate for the output video\n",
    "resize_size = 1800\n",
    "\n",
    "# Get frame size from the first image\n",
    "first_frame_path = os.path.join(frames_dir, frame_names[0])\n",
    "first_frame = Image.open(first_frame_path)\n",
    "composite_image = np.hstack((first_frame,first_frame))\n",
    "composite_image = imutils.resize(composite_image, width = resize_size)\n",
    "\n",
    "frame_height, frame_width, _ = composite_image.shape\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "def show_contours_on_frame(mask, frame):\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    frame_with_contours = cv2.drawContours(frame, contours, -1, (0, 255, 0), 10)\n",
    "    return frame_with_contours\n",
    "\n",
    "for out_frame_idx in tqdm(range(0, len(frame_names), vis_frame_stride)):\n",
    "    frame = cv2.imread(os.path.join(frames_dir, frame_names[out_frame_idx]))\n",
    "    result_frame = frame.copy()\n",
    "\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        mask = out_mask[0]\n",
    "\n",
    "        if mask.dtype == bool:\n",
    "            mask = mask.astype(np.uint8) * 255\n",
    "\n",
    "        _, binary_mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        result_frame = show_contours_on_frame(binary_mask, result_frame)\n",
    "\n",
    "    # Create a composite image by concatenating the original and detected images side by side\n",
    "    composite_image = np.hstack((frame,result_frame))\n",
    "    composite_image = imutils.resize(composite_image, width = resize_size)\n",
    "\n",
    "    # Add frame ID text in the top left corner\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    text = f\"Frame: {out_frame_idx}\"\n",
    "    position = (10, 30)  # Top left corner\n",
    "    font_scale = 1\n",
    "    font_color = (0, 255, 0)  # Green color in BGR\n",
    "    thickness = 2\n",
    "    line_type = cv2.LINE_AA\n",
    "\n",
    "    cv2.putText(composite_image, text, position, font, font_scale, font_color, thickness, line_type)\n",
    "    # Write the frame to the video\n",
    "    out.write(composite_image)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow('Contours on Frame', composite_image)\n",
    "    cv2.waitKey(10)\n",
    "\n",
    "# Release the VideoWriter object\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
